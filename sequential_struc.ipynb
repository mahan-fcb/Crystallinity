{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#!pip install prettytable\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "from IPython.display import Image\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import max_norm\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Activation, Reshape, Multiply\n",
    "from keras.layers import Conv1D, Add, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Embedding, Bidirectional, CuDNNLSTM, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.metrics\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(\"train_seq.p\")\n",
    "df_test = pd.read_pickle(\"test_seq.p\")\n",
    "dt = pd.read_pickle(\"train_biological.p\")\n",
    "dk = pd.read_pickle(\"test_biological.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train,df_test], axis=0)\n",
    "dt= pd.concat([dt,dk], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.DataFrame()\n",
    "k = df_train['Seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "k.to_csv('seq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3sn= pd.read_pickle(\"train_3s.p\")\n",
    "df_8sn = pd.read_pickle(\"train_8s.p\")\n",
    "df_3st = pd.read_pickle(\"test_3s.p\")\n",
    "df_8st = pd.read_pickle(\"test_8s.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3sn['solubility']=df_train['solubility']\n",
    "df_8sn['solubility']=df_train['solubility']\n",
    "df_3st['solubility']=df_test['solubility']\n",
    "df_8st['solubility']=df_test['solubility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dk['Str8_H'] \n",
    "del dk['Str8_C'] \n",
    "del dk['Str8_S'] \n",
    "del dk['Str8_E'] \n",
    "del dk['Str8_T'] \n",
    "del dk['Str8_G'] \n",
    "del dk['Str8_B'] \n",
    "del dk['Str8_I'] \n",
    "del dk['Str3_H'] \n",
    "del dk['Str3_E'] \n",
    "del dk['Str3_C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dk['Seq']\n",
    "del dk['3Str']\n",
    "del dk['8Str']\n",
    "del dt['Seq']\n",
    "del dt['3Str']\n",
    "del dt['8Str']\n",
    "del dk['Acc_Sol']\n",
    "del dt['Acc_Sol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing   # normalized traing data set\n",
    "x = dk.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "dk= pd.DataFrame(x_scaled)\n",
    "\n",
    "x = dt.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "dt = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= dt.to_numpy()\n",
    "X1 = dk.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_unique_cls(train, test):\n",
    "  \"\"\"\n",
    "  Prints # unique classes in data sets.\n",
    "  \"\"\"\n",
    "  train_unq = np.unique(train['solubility'].values)\n",
    "  #val_unq = np.unique(val['family_accession'].values)\n",
    "  test_unq = np.unique(test['solubility'].values)\n",
    "\n",
    "  #print('Number of unique classes in Train: ', len(train_unq))\n",
    "  #print('Number of unique classes in Val: ', len(val_unq))\n",
    "  #print('Number of unique classes in Test: ', len(test_unq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['seq_char_count']= df_train['Seq'].apply(lambda x: len(x))\n",
    "#df_val['seq_char_count']= df_val['sequence'].apply(lambda x: len(x))\n",
    "df_test['seq_char_count']= df_test['Seq'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3st['seq_char_count']= df_3st['3Str'].apply(lambda x: len(x))\n",
    "#df_val['seq_char_count']= df_val['sequence'].apply(lambda x: len(x))\n",
    "df_8st['seq_char_count']= df_8st['8Str'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3sn['seq_char_count']= df_3sn['3Str'].apply(lambda x: len(x))\n",
    "#df_val['seq_char_count']= df_val['sequence'].apply(lambda x: len(x))\n",
    "df_8sn['seq_char_count']= df_8sn['8Str'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df_train['solubility'].value_counts()[:2].index.tolist()\n",
    "#len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sm = df_train.loc[df_train['solubility'].isin(classes)].reset_index()\n",
    "#val_sm = df_val.loc[df_val['family_accession'].isin(classes)].reset_index()\n",
    "test_sm = df_test.loc[df_test['solubility'].isin(classes)].reset_index()\n",
    "\n",
    "#print('Data size after considering 1000 classes for each data split:')\n",
    "#print('Train size :', len(train_sm))\n",
    "#print('Val size :', len(val_sm))\n",
    "#print('Test size :', len(test_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sm1 = df_3sn.loc[df_3sn['solubility'].isin(classes)].reset_index()\n",
    "#val_sm = df_val.loc[df_val['family_accession'].isin(classes)].reset_index()\n",
    "test_sm1 = df_3st.loc[df_3st['solubility'].isin(classes)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sm2 = df_8sn.loc[df_8sn['solubility'].isin(classes)].reset_index()\n",
    "#val_sm = df_val.loc[df_val['family_accession'].isin(classes)].reset_index()\n",
    "test_sm2 = df_8st.loc[df_8st['solubility'].isin(classes)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>8Str</th>\n",
       "      <th>solubility</th>\n",
       "      <th>seq_char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CCHHCCTTCCCHHHCCHHHHHHHHHHHHHHHHHCSCBCHHHHHHHS...</td>\n",
       "      <td>1</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>CEEEEEEEECSSCSBSCEEEEEEEEEGGGHHHHHHTTGGGCTTSCH...</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>CCEEEEEEEEECSTTCEEEEETTEEEEECCSCTTTCCSCCHHHHHH...</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>CCCHCCSEEEECSCCCTTSCEEECCECCCHHHHHHHHHHHHTSSSC...</td>\n",
       "      <td>1</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>CCCCEEECSSTTEEEEECSSSCEEEEEECSSTTCCSSSCCEEEETT...</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               8Str  solubility  \\\n",
       "0      0  CCHHCCTTCCCHHHCCHHHHHHHHHHHHHHHHHCSCBCHHHHHHHS...           1   \n",
       "1      1  CEEEEEEEECSSCSBSCEEEEEEEEEGGGHHHHHHTTGGGCTTSCH...           1   \n",
       "2      2  CCEEEEEEEEECSTTCEEEEETTEEEEECCSCTTTCCSCCHHHHHH...           1   \n",
       "3      3  CCCHCCSEEEECSCCCTTSCEEECCECCCHHHHHHHHHHHHTSSSC...           1   \n",
       "4      4  CCCCEEECSSTTEEEEECSSSCEEEEEECSSTTCCSSSCCEEEETT...           1   \n",
       "\n",
       "   seq_char_count  \n",
       "0             338  \n",
       "1             230  \n",
       "2             141  \n",
       "3             193  \n",
       "4             230  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sm2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "         'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "def create_dict(codes):\n",
    "  char_dict = {}\n",
    "  for index, val in enumerate(codes):\n",
    "    char_dict[val] = index+1\n",
    "\n",
    "  return char_dict\n",
    "\n",
    "char_dict = create_dict(codes)\n",
    "\n",
    "#print(char_dict)\n",
    "#print(\"Dict Length:\", len(char_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes1 = ['H','E','S','T','C','G','B','I']\n",
    "\n",
    "def create_dict(codes1):\n",
    "  char_dict1 = {}\n",
    "  for index, val in enumerate(codes1):\n",
    "    char_dict1[val] = index+1\n",
    "\n",
    "  return char_dict1\n",
    "\n",
    "char_dict1 = create_dict(codes1)\n",
    "\n",
    "#print(char_dict)\n",
    "#print(\"Dict Length:\", len(char_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes2 = ['C','H','E']\n",
    "\n",
    "def create_dict(codes2):\n",
    "  char_dict2 = {}\n",
    "  for index, val in enumerate(codes2):\n",
    "    char_dict2[val] = index+1\n",
    "\n",
    "  return char_dict2\n",
    "\n",
    "char_dict2 = create_dict(codes2)\n",
    "\n",
    "#print(char_dict)\n",
    "#print(\"Dict Length:\", len(char_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encoding(data):\n",
    "  \"\"\"\n",
    "  - Encodes code sequence to integer values.\n",
    "  - 20 common amino acids are taken into consideration\n",
    "    and rest 4 are categorized as 0.\n",
    "  \"\"\"\n",
    "  \n",
    "  encode_list = []\n",
    "  for row in data['Seq'].values:\n",
    "    row_encode = []\n",
    "    for code in row:\n",
    "      row_encode.append(char_dict.get(code, 0))\n",
    "    encode_list.append(np.array(row_encode))\n",
    "  \n",
    "  return encode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encoding1(data):\n",
    "  \"\"\"\n",
    "  - Encodes code sequence to integer values.\n",
    "  - 20 common amino acids are taken into consideration\n",
    "    and rest 4 are categorized as 0.\n",
    "  \"\"\"\n",
    "  \n",
    "  encode_list1 = []\n",
    "  for row in data['3Str'].values:\n",
    "    row_encode1 = []\n",
    "    for code in row:\n",
    "      row_encode1.append(char_dict1.get(code, 0))\n",
    "    encode_list1.append(np.array(row_encode1))\n",
    "  \n",
    "  return encode_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encoding2(data):\n",
    "  \"\"\"\n",
    "  - Encodes code sequence to integer values.\n",
    "  - 20 common amino acids are taken into consideration\n",
    "    and rest 4 are categorized as 0.\n",
    "  \"\"\"\n",
    "  \n",
    "  encode_list2 = []\n",
    "  for row in data['8Str'].values:\n",
    "    row_encode2 = []\n",
    "    for code in row:\n",
    "      row_encode2.append(char_dict2.get(code, 0))\n",
    "    encode_list2.append(np.array(row_encode2))\n",
    "  \n",
    "  return encode_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encode = integer_encoding(train_sm) \n",
    "#val_encode = integer_encoding(val_sm) \n",
    "test_encode = integer_encoding(test_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encode1 = integer_encoding1(train_sm1) \n",
    "#val_encode = integer_encoding(val_sm) \n",
    "test_encode1 = integer_encoding1(test_sm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encode2 = integer_encoding2(train_sm2) \n",
    "#val_encode = integer_encoding(val_sm) \n",
    "test_encode2 = integer_encoding2(test_sm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding sequences\n",
    "\n",
    "max_length =100\n",
    "train_pad = pad_sequences(train_encode, maxlen=max_length, padding='post', truncating='post')\n",
    "#val_pad = pad_sequences(val_encode, maxlen=max_length, padding='post', truncating='post')\n",
    "test_pad = pad_sequences(test_encode, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "#train_pad.shape, test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding sequences\n",
    "\n",
    "max_length =80\n",
    "train_pad1 = pad_sequences(train_encode1, maxlen=max_length, padding='post', truncating='post')\n",
    "#val_pad = pad_sequences(val_encode, maxlen=max_length, padding='post', truncating='post')\n",
    "test_pad1 = pad_sequences(test_encode1, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "#train_pad.shape, test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding sequences\n",
    "\n",
    "max_length =80\n",
    "train_pad2 = pad_sequences(train_encode2, maxlen=max_length, padding='post', truncating='post')\n",
    "#val_pad = pad_sequences(val_encode, maxlen=max_length, padding='post', truncating='post')\n",
    "test_pad2 = pad_sequences(test_encode2, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "#train_pad.shape, test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ohe = to_categorical(train_pad)\n",
    "#val_ohe = to_categorical(val_pad)\n",
    "test_ohe = to_categorical(test_pad)\n",
    "\n",
    "#train_ohe.shape, test_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ohe1 = to_categorical(train_pad1)\n",
    "#val_ohe = to_categorical(val_pad)\n",
    "test_ohe1 = to_categorical(test_pad1)\n",
    "\n",
    "#train_ohe.shape, test_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ohe2 = to_categorical(train_pad2)\n",
    "#val_ohe = to_categorical(val_pad)\n",
    "test_ohe2 = to_categorical(test_pad2)\n",
    "\n",
    "#train_ohe.shape, test_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label/integer encoding output variable: (y)\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_train_le = le.fit_transform(train_sm['solubility'])\n",
    "#y_val_le = le.transform(val_sm['family_accession'])\n",
    "y_test_le = le.fit_transform(test_sm['solubility'])\n",
    "\n",
    "#y_train_le.shape, y_test_le.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of outputs\n",
    "y_train = to_categorical(y_train_le)\n",
    "#y_val = to_categorical(y_val_le)\n",
    "y_test = to_categorical(y_test_le)\n",
    "\n",
    "#y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function: plot model's accuracy and loss\n",
    "\n",
    "# https://realpython.com/python-keras-text-classification/\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  x = range(1, len(acc) + 1)\n",
    "\n",
    "  plt.figure(figsize=(12, 5))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(x, acc, 'b', label='Training acc')\n",
    "  plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(x, loss, 'b', label='Training loss')\n",
    "  plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_score(model, train, val, test, batch_size):\n",
    "\n",
    "  train_score = model.evaluate(train[0], train[1], batch_size=batch_size, verbose=1)\n",
    "  print('Train loss: ', train_score[0])\n",
    "  print('Train accuracy: ', train_score[1])\n",
    "  print('-'*70)\n",
    "\n",
    "  val_score = model.evaluate(val[0], val[1], batch_size=batch_size, verbose=1)\n",
    "  print('Val loss: ', val_score[0])\n",
    "  print('Val accuracy: ', val_score[1])\n",
    "  print('-'*70)\n",
    "  \n",
    "  test_score = model.evaluate(test[0], test[1], batch_size=batch_size, verbose=1)\n",
    "  print('Test loss: ', test_score[0])\n",
    "  print('Test accuracy: ', test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SE_block(input_shape):\n",
    "    x = GlobalMaxPooling1D()(input_shape)\n",
    "    x = Dense(int(int(input_shape.shape[-1])/8),activation='relu')(x)\n",
    "    x = Dense(int(input_shape.shape[-1]),activation='sigmoid')(x)\n",
    "    x = Reshape((-1,int(input_shape.shape[-1])))(x)\n",
    "    #x = Concatenate()([x, input_shape])\n",
    "    x = Multiply()([x, input_shape])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(data, filters, d_rate):\n",
    "  \"\"\"\n",
    "  _data: input\n",
    "  _filters: convolution filters\n",
    "  _d_rate: dilation rate\n",
    "  \"\"\"\n",
    "\n",
    "  shortcut = data\n",
    "\n",
    "  bn1 = BatchNormalization()(data)\n",
    "  act1 = Activation('relu')(bn1)\n",
    "  conv1 = Conv1D(filters, 1, dilation_rate=d_rate, padding='same', kernel_regularizer=l2(0.001))(act1)\n",
    "\n",
    "  #bottleneck convolution\n",
    "  bn2 = BatchNormalization()(conv1)\n",
    "  act2 = Activation('relu')(bn2)\n",
    "  conv2 = Conv1D(filters, 3, padding='same', kernel_regularizer=l2(0.001))(act2)\n",
    "  \n",
    "\n",
    "  #bn3 = BatchNormalization()(conv2)\n",
    "  #act3 = Activation('relu')(bn3)\n",
    "  #conv3 = Conv1D(filters, 4, padding='same', kernel_regularizer=l2(0.001))(act3)\n",
    "  conv = SE_block(conv2)\n",
    "  #skip connection\n",
    "  x = Add()([conv, shortcut])\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10194 samples, validate on 1134 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv1d_6/convolution (defined at C:\\Users\\moham\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_7254]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-287533bf09d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'loss_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m#model2.summary()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mhis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv1d_6/convolution (defined at C:\\Users\\moham\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_7254]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "kfold = 10\n",
    "\n",
    "\n",
    "random_state = 1\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "save_dir = '/saved_models/'\n",
    "t = np.zeros(kfold)\n",
    "#a = np.zeros(kfold)\n",
    "b = np.zeros(kfold)\n",
    "#c = np.zeros(kfold)\n",
    "#d = np.zeros(kfold)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)\n",
    "k = 0\n",
    "epochs =50\n",
    "batch_size =128\n",
    "\n",
    "for train_index, test_index in skf.split(train_ohe, y_train_le):\n",
    "    X_train, X_test = train_ohe[train_index], train_ohe[test_index]\n",
    "   # X_train3, X_test3 = train_ohe1[train_index], train_ohe1[test_index]\n",
    "    #X_train4, X_test4 = train_ohe2[train_index], train_ohe2[test_index]\n",
    "    X_train2, X_test2 = X[train_index], X[test_index]\n",
    "    y_train, y_test = y_train_le[train_index], y_train_le[test_index]\n",
    "\n",
    "    optimizer = Adam(lr=0.001, decay=1e-8)\n",
    "    x_input = Input(shape=(800, 21))\n",
    "    #x_input1 = Input(shape=(80,6))\n",
    "    #x_input2 = Input(shape=(80,4))\n",
    "    input2 = Input(shape=(62,))\n",
    "\n",
    "#initial conv\n",
    "    conv = Conv1D(128, 1, padding='same')(x_input) \n",
    "\n",
    "# per-residue representation\n",
    "    res1 = residual_block(conv, 128, 2)\n",
    "    res2 = residual_block(res1, 128, 3)\n",
    "\n",
    "    x = MaxPooling1D(3)(res2)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "# softmax classifier\n",
    "    x = Flatten()(x)\n",
    "    #conv1 = Conv1D(20, 1, padding='same')(x_input1) \n",
    "\n",
    "# per-residue representation\n",
    "    #res11 = residual_block(conv1, 20, 2)\n",
    "    #res21 = residual_block(res11, 20, 3)\n",
    "\n",
    "   # x1 = MaxPooling1D(3)(res21)\n",
    "   # x1 = BatchNormalization()(x1)\n",
    "    #x1 = Dropout(0.5)(x1)\n",
    "\n",
    "# softmax classifier\n",
    "    #x1 = Flatten()(x1)\n",
    "    #conv2 = Conv1D(20, 1, padding='same')(x_input2) \n",
    "\n",
    "# per-residue representation\n",
    "    #res12 = residual_block(conv2, 20, 2)\n",
    "    #res22 = residual_block(res12, 20, 3)\n",
    "\n",
    "    #x2 = MaxPooling1D(3)(res22)\n",
    "    #x2 = BatchNormalization()(x2)\n",
    "    #x2 = Dropout(0.5)(x2)\n",
    "\n",
    "# softmax classifier\n",
    "    #x2 = Flatten()(x2)\n",
    "    x = concatenate([x, input2])\n",
    "    #x1= concatenate([x1, x])\n",
    "    #x2= concatenate([x2,x1])\n",
    "    x_output = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0001))(x)\n",
    " \n",
    "    model2 = Model(inputs=[x_input,input2], outputs=x_output)\n",
    "    model2.compile(optimizer=optimizer, loss= 'binary_crossentropy', metrics=['accuracy'])\n",
    "    es = EarlyStopping(monitor='loss_accuracy', patience=5, verbose=1)\n",
    "    #model2.summary()\n",
    "    his = model2.fit([X_train,X_train2], y_train ,batch_size=batch_size,  epochs=epochs, validation_data= ([X_test,X_test2], y_test),verbose=1,  shuffle=True)\n",
    "\n",
    "\n",
    "    y_pred = model2.predict([test_ohe, X1]).round().astype(int)\n",
    "    y_train_pred = model2.predict([X_train, X_train2]).round().astype(int)\n",
    "\n",
    "    t[k] = sklearn.metrics.f1_score(y_test_le, y_pred, average='weighted')\n",
    "    # Confusion Matrix\n",
    "    #print(t[k], b[k])\n",
    "# Accuracy\n",
    "    b[k] = accuracy_score(y_test_le, y_pred)\n",
    "    print(t[k], b[k])\n",
    "# Recall\n",
    "    #c[k] = recall_score(y_test, y_pred, average=None)\n",
    "    model2.save('model4_'+str(k)+\".h5\")\n",
    "    results = model2.evaluate([test_ohe, X1], y_test_le)\n",
    "    results = dict(zip(model2.metrics_names,results))\n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "# Precision\n",
    "\n",
    "    k+=1\n",
    "    \n",
    "print ('Average f1 score', np.mean(t))\n",
    "print ('Average ac score', np.mean(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
